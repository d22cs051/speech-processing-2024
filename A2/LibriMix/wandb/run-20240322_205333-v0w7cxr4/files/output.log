SepformerSeparation(
  (mods): ModuleDict(
    (encoder): Encoder(
      (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(8,), bias=False)
    )
    (decoder): Decoder(256, 1, kernel_size=(16,), stride=(8,), bias=False)
    (masknet): Dual_Path_Model(
      (norm): GroupNorm(1, 256, eps=1e-08, affine=True)
      (conv1d): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (dual_mdl): ModuleList(
        (0-1): 2 x Dual_Computation_Block(
          (intra_mdl): SBTransformerBlock(
            (mdl): TransformerEncoder(
              (layers): ModuleList(
                (0-7): 8 x TransformerEncoderLayer(
                  (self_att): MultiheadAttention(
                    (att): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (pos_ffn): PositionalwiseFeedForward(
                    (ffn): Sequential(
                      (0): Linear(in_features=256, out_features=1024, bias=True)
                      (1): ReLU()
                      (2): Dropout(p=0, inplace=False)
                      (3): Linear(in_features=1024, out_features=256, bias=True)
                    )
                  )
                  (norm1): LayerNorm(
                    (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  )
                  (norm2): LayerNorm(
                    (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  )
                  (dropout1): Dropout(p=0, inplace=False)
                  (dropout2): Dropout(p=0, inplace=False)
                )
              )
              (norm): LayerNorm(
                (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              )
            )
            (pos_enc): PositionalEncoding()
          )
          (inter_mdl): SBTransformerBlock(
            (mdl): TransformerEncoder(
              (layers): ModuleList(
                (0-7): 8 x TransformerEncoderLayer(
                  (self_att): MultiheadAttention(
                    (att): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                    )
                  )
                  (pos_ffn): PositionalwiseFeedForward(
                    (ffn): Sequential(
                      (0): Linear(in_features=256, out_features=1024, bias=True)
                      (1): ReLU()
                      (2): Dropout(p=0, inplace=False)
                      (3): Linear(in_features=1024, out_features=256, bias=True)
                    )
                  )
                  (norm1): LayerNorm(
                    (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  )
                  (norm2): LayerNorm(
                    (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  )
                  (dropout1): Dropout(p=0, inplace=False)
                  (dropout2): Dropout(p=0, inplace=False)
                )
              )
              (norm): LayerNorm(
                (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              )
            )
            (pos_enc): PositionalEncoding()
          )
          (intra_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
          (inter_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
        )
      )
      (conv2d): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))
      (end_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (prelu): PReLU(num_parameters=1)
      (activation): ReLU()
      (output): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        (1): Tanh()
      )
      (output_gate): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        (1): Sigmoid()
      )
    )
  )
)
Epoch Number::   0%|                                                                     | 0/10 [00:00<?, ?it/s]

  return F.conv1d(input, weight, bias, self.stride,                                      | 0/38 [00:00<?, ?it/s]
Testing Batch:   0%|                                                                     | 0/38 [00:03<?, ?it/s]
Epoch Number::   0%|                                                                     | 0/10 [00:03<?, ?it/s]
[34m[1mwandb[39m[22m: [32m[41mERROR[39m[49m Raw audio requires the soundfile package. To get it, run "pip install soundfile"
Traceback (most recent call last):
  File "/DATA1/bikash_dutta/CS/SP/A2/LibriMix/test.py", line 189, in <module>
    data = [[wandb.Audio(mix[0],sample_rate=16000), wandb.Audio(s1[0],sample_rate=16000), wandb.Audio(s2[0],sample_rate=16000), wandb.Audio(est_sources[:,:,0][0],sample_rate=16000), wandb.Audio(est_sources[:,:,1][0],sample_rate=16000)]]
  File "/home/curie/miniconda3/envs/sb/lib/python3.10/site-packages/wandb/data_types.py", line 1012, in __init__
    soundfile = util.get_module(
  File "/home/curie/miniconda3/envs/sb/lib/python3.10/site-packages/wandb/util.py", line 272, in get_module
    raise wandb.Error(required)
wandb.errors.Error: Raw audio requires the soundfile package. To get it, run "pip install soundfile"