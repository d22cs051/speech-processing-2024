Epoch Number::   0%|          | 0/15 [00:00<?, ?it/s]









































































































































































































































































































































































































































































































































































































































































































































































































Batch Number:: 100%|██████████| 782/782 [45:58<00:00,  2.86s/it]
Batch Number::   0%|          | 0/782 [00:00<?, ?it/s]

















































































Epoch Number::   0%|          | 0/15 [50:40<?, ?it/s] 3.64s/it]
Traceback (most recent call last):
  File "fine-tune-wavelm-base-plus.py", line 204, in <module>
    output = pretrained_model(wav1, wav2)
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "fine-tune-wavelm-base-plus.py", line 109, in forward
    audio1_emb = self.feature_extractor(auido1)
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/downstreams/speaker_verification/models/ecapa_tdnn.py", line 274, in forward
    x = self.get_feat(x)
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/downstreams/speaker_verification/models/ecapa_tdnn.py", line 255, in get_feat
    x = self.feature_extract([sample for sample in x])
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/s3prl/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/s3prl/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/s3prl/s3prl/upstream/wavlm/WavLM.py", line 366, in extract_features
    features = self.feature_extractor(source)
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/s3prl/s3prl/upstream/wavlm/WavLM.py", line 525, in forward
    x = conv(x)
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA1/bikash_dutta/CS/SP/A2/UniSpeech/s3prl/s3prl/upstream/wavlm/modules.py", line 51, in forward
    output = F.group_norm(
  File "/home/curie/miniconda3/envs/sp/lib/python3.8/site-packages/torch/nn/functional.py", line 2359, in group_norm
    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: CUDA out of memory. Tried to allocate 2.37 GiB (GPU 0; 23.69 GiB total capacity; 9.66 GiB already allocated; 997.62 MiB free; 20.66 GiB reserved in total by PyTorch)